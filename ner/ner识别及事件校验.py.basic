#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time : 2023/5/4 10:16 
# @Author : huxiaoliang
# @Description : basic:对每个关键词文件操作
# @See：

# 关闭警告
import warnings

warnings.filterwarnings("ignore")

import csv
import time
import re
from hanlp_restful import HanLPClient
from api_demo.api_demo.utils import file_util
from api_demo.api_demo.utils.mylogger import Logger
import more_itertools

# 创建客户端，填入服务器地址和秘钥：
url = 'https://www.hanlp.com/api'
HanLP = HanLPClient(url, auth='MjMzNkBiYnMuaGFubHAuY29tOjVPaGpUeUI2VkJBVzR6Vmo=', language='zh')


log = Logger('../log/all.log', level='info')
# Logger('../log/error.log', level='error').logger.error('error')


# res =HanLP('我在上海林原科技有限公司兼职工作，我经常在台川喜宴餐厅吃饭，偶尔去开元地中海影城看电影。', tasks='ner/msra')
# print(res)
def is_org(val):
    if val[1] == 'ORGANIZATION':
        return True
    else:
        return False


# is_org测试


# 读取数据，对爬取的资讯过滤并提取公司名
keywords = [
    '流动性困难',
    '资金链断裂',
    '供应链中断',  # 多数为宏观政策、行业、国际；标题中未识别出实体
    '违规担保',
    # '担保违约',
    # '债务风险',
    # '债务违约'
    # 'CEO辞职',
    # '董事长辞职',
    # '高管辞职',
    # '总裁辞职',
    # '董秘辞职',
    # '董事辞职'
]

# fixme:(.*)->(.{0,10})-> .{0,10}指定字符个数
keyword_pattrens = {
    '流动性困难': r'流动性.{0,10}(困难|告急|不足|压力|不足|问题|困境|危机)',
    '资金链断裂': r'资金.{0,10}(断裂|困难|告急|不足|压力|不足|问题|困境|危机)',
    '供应链中断': r'供应.{0,30}(中断|生变|限制|风险|重创|问题|紧张|无法.{0,5}恢复)',  # 多数为宏观政策、行业、国际；标题中未识别出实体
    '违规担保': r'违规担保',

}
sources = [
    'baijiahao',
    'xueqiu',
]
# for keyword in keywords:
for keyword in keyword_pattrens.keys():
    for source in sources:
        with open(r'D:\work_softs\workspace-py-citic\industry_insight\news_spider\{source}\{keyword}.csv'.format(
                source=source,
                keyword=keyword), 'r', newline='',
                encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile, delimiter='`')
            field_names = ['标题',
                           'unique_orgs',
                           'analy',
                           'title_analy',
                           'abstract_analy',
                           'title_orgs',
                           '摘要',
                           'abstract_orgs',
                           '新闻来源',
                           '发布时间',
                           '链接']
            writer = file_util.get_dictwriter(
                r'D:\work_softs\workspace-py-citic\industry_insight\data\ner_reslut\{source}'.format(source=source),
                field_names, keyword)
            log.logger.info('开始处理:{}-{}...'.format(source,keyword))
            match_cnt = 0
            match_org_cnt = 0
            cnt = 0
            pos_neg_cnt = {0: 0, 1: 0, 2: 0}
            for row in reader:
                cnt +=1
                # 输出查看匹配规则
                # print(row['标题'], row['新闻来源'], row['发布时间'], row['链接'], '\n', row['摘要'])
                # continue
                # filter rule1:关键词命中标题or摘要
                # eg：流动性困难：标题或摘要 命中“流动性困难”，或命中“流动性”+“困难、告急、不足、压力”
                # pattern = r'流动性.{0,10}(困难|告急|不足|压力|不足|问题|困境|危机)'
                pattern = keyword_pattrens[keyword]
                title_match_obj = re.search(pattern, row['标题'])
                abstract_match_obj = re.search(pattern, row['摘要'])
                if title_match_obj or abstract_match_obj:
                    match_cnt += 1
                    # print(row['链接'],len(row['标题']),row['标题'], len(row['摘要']),row['摘要'], len(row['正文']),row['正文'], sep='\n')
                    # filter rule2:国际or国内；宏光政策、行业or企业 TODO:提供的关键词字典即为企业资讯
                    # ner rule：# 对标题、摘要 识别组织机构:只识别标题也可，因为重要新闻往往会重复，
                    # ress:结构
                    ress = HanLP(row['标题'], tasks='ner/msra')['ner/msra']
                    title_analy = round(HanLP.sentiment_analysis(row['标题']), 2)
                    abstract_analy = round(HanLP.sentiment_analysis(row['摘要']), 2)
                    # 正负面：同负；一正一负和小于0；一正一负和大于0；同正
                    log.logger.info("基本信息：{} {} {} {} {} {}".format(row['标题'], title_analy, abstract_analy, row['新闻来源'], row['发布时间'], row['链接']))
                    # print(ress)
                    title_orgs = []
                    unique_orgs = set()
                    for res in ress:
                        rts = list(filter(is_org, res))
                        if rts:
                            title_orgs.append(rts)
                            for rt in rts:
                                if rt:
                                    unique_orgs.add(rt[0])
                    ress = HanLP(row['摘要'], tasks='ner/msra')['ner/msra']
                    log.logger.info("abstract:{}".format(row['摘要']))
                    # print('title_orgs: {}'.format(title_orgs))
                    log.logger.info('title_orgs: {}'.format(title_orgs))
                    # print(ress)
                    abstract_orgs = []
                    for res in ress:
                        rts = list(filter(is_org, res))
                        if rts:
                            abstract_orgs.append(rts)
                            for rt in rts:
                                if rt:
                                    unique_orgs.add(rt[0])
                    log.logger.info('abstract_orgs:{}'.format(abstract_orgs))
                    log.logger.info('unique_orgs: {}'.format(unique_orgs))
                    time.sleep(8)

                    # 将最终结果输出到对应文件
                    # 实体提取结果处理：
                    if len(title_orgs) > 0 or len(abstract_orgs) > 0:
                        match_org_cnt += 1
                        # 对提取结果进行过滤

                    # 正负向输出
                    tag = ''
                    if title_analy < 0 and abstract_analy < 0:
                        pos_neg_cnt[0] += 1
                        tag = "双负"
                    elif title_analy > 0 and abstract_analy > 0:
                        pos_neg_cnt[2] += 1
                        tag = "双正"
                    else:
                        pos_neg_cnt[1] += 1
                        tag = "一正一负"

                    dic = {
                        '标题': row['标题'],
                        'unique_orgs': unique_orgs,
                        'analy': tag,
                        'title_analy': title_analy,
                        'abstract_analy': abstract_analy,
                        'title_orgs': title_orgs,
                        '摘要': row['摘要'],
                        'abstract_orgs': abstract_orgs,
                        '新闻来源': row['新闻来源'],
                        '发布时间': row['发布时间'],
                        '链接': row['链接']
                    }
                    log.logger.info('情感分类:{}'.format(tag))
                    writer.writerow(dic)
                    # print("dic:{}".format(dic))
                    log.logger.info("dic:{}".format(dic))
                    flag = False
                    if len(unique_orgs)>0 and tag in ['双负','一正一负']:
                        print("dic:{}".format(dic))
                        flag = True
                    log.logger.info("是否满足最终输出：{}".format(flag))
                    log.logger.info('=' * 300)


            # 匹配数量输出；标题/摘要实体提取输出；三个为负向数量输出；人工核对数量食输出
            print('{}-{} 处理完毕，总量：{}，匹配量：{}，可提取org量：{}，均为负：{}，一正一负：{}，均为正：{}'.format(keyword,source, cnt, match_cnt,
                                                                                 match_org_cnt, pos_neg_cnt[0],
                                                                                 pos_neg_cnt[1], pos_neg_cnt[2]))
            log.logger.info('{}-{} 处理完毕，总量：{}，匹配量：{}，可提取org量：{}，均为负：{}，一正一负：{}，均为正：{}'.format(keyword,source, cnt, match_cnt,
                                                                                 match_org_cnt, pos_neg_cnt[0],
                                                                                 pos_neg_cnt[1], pos_neg_cnt[2]))
            # 观察结果，修正规则；
            # 四个关键词，50*4；结果统计；观察新规则
            # TODO:
            # 目前代码不适合雪球号
            # 不唯一的公司实体
            # eg-通过距离/黑名单解决：基本信息：重庆信托:因债务人流动性困难,昆明融创城集合资金计划实质违约
            # eg-通过黑名单解决：unique_orgs: {'半年', '搜于特集团股份有限公司', '深圳证券交易所'}
            # eg-优先标题中的实体/黑名单/高频重复（标题，正文）:基本信息：隆鑫系两A股公司扣非连续三年下降 涂建华累计逾期债务近150亿财务...
            # 黑名单之一：资讯来源，交易所，信托机构，确认不是org得badcase
            # 可能需要匹配不同得ner识别得api
            black_list = ['教育']#重庆信托 澎湃新闻 财联社 深圳证券交易所 银保监会  华夏时报 浙江证监局 网易财经 美国半导体行业协会 纽约联储 多米诺骨牌 专网 陕西监管局 国务院 XX证券 '证券日报' '立信会计师事务所'
            badcase = ['结婚产业观察','关联'] #陕西县委  交易所 董事会 深交所 中国证监会 V观财报 *
            # 类型判断：中文，数字，字母
            # 选择长的：{'st中利', 'st', '中利'}
            # 实体重复
            # eg:unique_orgs: {'东旭光电', '东旭光电董事会'}
            # 跟事件名的先后关系：

            # NER提取公司组织汇总：
            # 1、标题中NER优先；+文字字符距离；+黑名单过滤/badcase过滤
            # 2、文字字符距离；+黑名单过滤/badcase过滤；
            # 匹配规则

            # 时间效率：
            # 只对双负，一正一负得操作




# NER识别+规则
# 记录：雪球
# 雪球网信息比较杂（10之1 2关键词为主要意思）：比如CEO辞职，多数为为国际新闻，多数CEO辞职语义，命中的重复多（自媒体人多）
# 董事长辞职 命中的多为董事长，语义不合
# 流动性困难 国际新闻巨多

# 规则：
# 命中标题（高优），摘要（次优），全文（低优）严格命中，命中分开的关键词 同语义关键词
# 国内新闻or国际新闻判断：eg：来源？
# 目标：国内新闻，中小企业，语义匹配

# 百度百家号
# 标题中完全命中的并不在最前面，是按焦点排序的；调整为按时间排序会好一点，同时按时间排序容易做增量，
# 选择按时间排序，按焦点的话不好做增量，同时，非语义匹配的，因为焦点高而排在前


# 分析百家号：流动性**处理结果
# 标题中基本均包含公司实体名，未识别出来的基本是政策或行业资讯。有部分丢失，也能因为重复新闻得到补充。
# 建议规则：标题和摘要识别关键字过滤数据，公司主体从标题中识别

# 爬虫 TODO：将资讯时间计算一下
